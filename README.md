# Проект анализа тональности комментариев YouTube с использованием PySpark и языковых моделей

Этот проект представляет собой пайплайн для анализа тональности (sentiment analysis) комментариев YouTube. Он использует современные методы обработки естественного языка (NLP) для преобразования текста в семантические векторы (эмбеддинги) и обучает на них классификационную модель с помощью Apache Spark.

## Оглавление
- [Цель проекта](#цель-проекта)
- [Используемые технологии](#используемые-технологии)
- [Структура проекта](#структура-проекта)
- [Как запустить](#как-запустить)
- [Описание пайплайна](#описание-пайплайна)
  - [1. Предобработка данных](#1-предобработка-данных)
  - [2. Генерация эмбеддингов](#2-генерация-эмбеддингов)
  - [3. Обучение классификатора](#3-обучение-классификатора)
- [Дальнейшие шаги](#дальнейшие-шаги)

## Цель проекта

Основная цель — разработать масштабируемую модель машинного обучения, способную классифицировать текстовые комментарии на три категории: **положительные**, **нейтральные** и **отрицательные**.

## Используемые технологии

- **Python 3**: Основной язык программирования.
- **Apache Spark (PySpark)**: Фреймворк для распределенной обработки данных и обучения моделей машинного обучения.
- **Sentence-Transformers**: Библиотека для генерации state-of-the-art текстовых эмбеддингов с использованием предобученных языковых моделей.
- **Pandas**: Используется для удобной обработки данных перед генерацией эмбеддингов.
- **Rye**: Современный инструмент для управления зависимостями и виртуальным окружением в Python.

## Структура проекта

```
BD_proj/
│
├── .venv/                      # Виртуальное окружение, управляемое Rye
├── artifacts/                  # Директория для сохранения артефактов (например, графиков)
├── processed_comments.parquet/ # Предобработанные данные в формате Parquet
├── sentiment_analysis_model/   # Сохраненная обученная модель
│
├── .gitignore                  # Файл для исключения файлов из Git
├── kaggle.json                 # (Опционально) Ключи для Kaggle API
├── preprocess_data.py          # Скрипт для начальной очистки и подготовки данных
├── pyproject.toml              # Основной файл конфигурации проекта для Rye
├── README.md                   # Этот файл
├── requirements-dev.lock       # Файл зависимостей для разработки
├── requirements.lock           # Файл зависимостей для production
├── run.sh                      # Главный исполняемый скрипт для запуска всего пайплайна
├── train_model.py              # Основной скрипт для генерации эмбеддингов и обучения модели
└── visualize_sentiment.py      # Скрипт для создания визуализаций
```

## Как запустить

### Предварительные требования

1.  **Установленный Rye**: Инструкции по установке можно найти на [официальном сайте](https://rye-up.com/).
2.  **Исходные данные**: Убедитесь, что в корневой директории проекта находится файл с данными `YoutubeCommentsDataSet.csv`.

### Инструкции по запуску

1.  **Клонируйте репозиторий**:
    ```bash
    git clone <your-repo-url>
    cd BD_proj
    ```

2.  **Синхронизируйте зависимости**: Эта команда установит все необходимые библиотеки, указанные в `pyproject.toml`, в локальное виртуальное окружение.
    ```bash
    rye sync
    ```

3.  **Сделайте скрипт запуска исполняемым**:
    ```bash
    chmod +x run.sh
    ```

4.  **Запустите пайплайн**:
    ```bash
    ./run.sh
    ```
    Скрипт последовательно выполнит все необходимые шаги: предобработку данных (если она не была выполнена ранее) и основной цикл обучения и оценки модели. В первый раз будет скачана предобученная языковая модель, что может занять несколько минут.

## Описание пайплайна

Процесс разделен на три основных этапа, которые выполняются скриптами `preprocess_data.py` и `train_model.py`.

### 1. Предобработка данных

-   **Скрипт**: `preprocess_data.py`
-   **Действия**:
    -   Загружает исходный CSV-файл с помощью PySpark.
    -   Проводит очистку текста: удаляет знаки препинания, специальные символы и приводит текст к нижнему регистру.
    -   Производит **токенизацию** (разбиение текста на отдельные слова) и удаляет **стоп-слова** (часто встречающиеся слова, не несущие смысловой нагрузки, например "a", "the", "is").
    -   Сохраняет очищенные и подготовленные данные в формате `parquet` в директорию `processed_comments.parquet/` для дальнейшего использования.

### 2. Генерация эмбеддингов

-   **Скрипт**: `train_model.py`
-   **Подход**: Вместо классических методов, таких как TF-IDF, которые просто считают частоту слов, мы используем современную языковую модель **`all-MiniLM-L6-v2`** из библиотеки `sentence-transformers`.
-   **Преимущества**:
    -   **Понимание контекста**: Модель улавливает семантическое значение слов и предложений. Слова "отличный" и "превосходный" будут иметь близкие векторные представления.
    -   **Высокое качество**: Эмбеддинги, полученные от предобученных моделей, значительно повышают качество классификации по сравнению с традиционными подходами.
-   **Процесс**:
    1.  Данные из `processed_comments.parquet` загружаются в `pandas.DataFrame`.
    2.  Модель `SentenceTransformer` преобразует каждый комментарий в 384-мерный вектор (эмбеддинг).
    3.  Результат (пары "эмбеддинг-метка") конвертируется обратно в Spark DataFrame.

### 3. Обучение классификатора

-   **Скрипт**: `train_model.py`
-   **Алгоритм**: На полученных семантических эмбеддингах обучается модель `LogisticRegression` из библиотеки `PySpark ML`.
-   **Процесс**:
    -   Данные разделяются на обучающую (80%) и тестовую (20%) выборки.
    -   Модель логистической регрессии обучается на обучающей выборке.
    -   Производится оценка точности (`accuracy`) на тестовой выборке.
    -   Финальная точность выводится в консоль, а обученная модель сохраняется в директорию `embedding_logreg_model/`.

## Дальнейшие шаги

-   **Эксперименты с моделями эмбеддингов**: Можно попробовать другие, более крупные и мощные модели из `sentence-transformers` для потенциального улучшения качества эмбеддингов.
-   **Тонкая настройка (Fine-tuning)**: Для достижения максимальной точности можно провести fine-tuning языковой модели непосредственно на данных комментариев.
-   **Подбор гиперпараметров**: Провести кросс-валидацию для подбора оптимальных параметров `LogisticRegression` (`regParam`, `elasticNetParam`).
-   **Развертывание (Deployment)**: Обернуть обученную модель в API для использования в реальных приложениях.
